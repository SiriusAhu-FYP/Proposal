\section{Introduction}
% è®²æ•…äº‹ä¸»çº¿ï¼šå…ˆè¯´``ä¸ºä»€ä¹ˆéœ€è¦/ç°åœ¨ç¼ºä»€ä¹ˆ''ï¼Œå†è¯´``æˆ‘ä»¬é€‰æ‹©ä»€ä¹ˆè·¯å¾„ä¸å£å¾„''ï¼Œç»™å‡º``ç³»ç»Ÿé¢„è§ˆä¸è®¾è®¡åŸåˆ™''ï¼Œç‚¹å‡º``å…³é”®æŒ‘æˆ˜''ï¼Œæœ€åæ˜ç¡®``ç›®æ ‡/äº¤ä»˜ç‰©/è¾¹ç•Œ''ã€‚å¼•ç”¨ä»¥ä»£è¡¨æ€§è®ºæ–‡ä¸ºè¯ï¼Œä¸å †æ ˆã€‚

% ==========
% 1.1 Problem Statement & Motivation
% ==========
\subsection{Problem Statement \& Motivation}
% Why worthy? (Passion, market, need)
% Demand -> Neuro-sama (success on market and tech) -> Timely: recent research support -> Project Definition

The demand for \textbf{interactive and companion-like experiences} in real-time entertainment is rapidly growing, with players seeking more than just simple automation or static overlays. As the landscape of interactive experiences evolves, the focus is shifting towards genuine engagement and companionship, with players desiring dynamic, responsive partners that enhance their overall experience.

On the one hand, games like AI2U: ``With You 'Til The End'' are already capitalizing on this trend, drawing players in with the novelty of engaging with generative AI characters \cite{ai2u_game}. At the same time, larger companies are investing in foundational technologies like NVIDIA's ACE (Avatar Cloud Engine) and Ubisoft's ``NEO NPCs'', aiming to create autonomous agents that enhance gameplay rather than just drive conversation~\cite{nvidia_ace_autonomous_2025, ubisoft_neo_npc_2024}. These shifts in the gaming industry are validating the commercial viability of AI-driven experiences, as evidenced by increased player engagement and media interest~\cite{inworld_market_validation_2023}.

On the other hand, the rise of AI-driven virtual streamers, particularly the Neuro-sama phenomenon, highlights a significant shift in both technology and community-driven commercialization. Neuro-sama, an AI-powered virtual streamer (VTuber), engages in real-time conversations and dynamic gameplay, capturing the attention of audiences \cite{neurosama_youtube, streamelements_state_2024, streamscharts_q4_2024_landscape}. Although Neuro-sama remains closed-source, its success has inspired a vibrant open-source ecosystem, with developers aiming to replicate or expand upon its capabilities \cite{open_llm_vtuber, airi_project, kimjammer_neuro}. This technological innovation has been accompanied by strong commercial traction on platforms like Twitch, indicating a growing market demand for AI that offers both utility and companionship \cite{neurosama_hype_streamscharts_2025}.

This project is timely. While the market demand is clear, the technological feasibility for a \textbf{reproducible, non-API-dependent} assistant has only recently emerged. Foundational research is now converging on the key methodologies required for productization. This includes the development of \textbf{unified evaluation protocols} and \textbf{modular ablation frameworks}, which are essential for ensuring that experiments are reproducible and comparable \cite{ORAK, lmgame-bench}. Furthermore, recent work has demonstrated the viability of \textbf{General Computer Control (GCC)} pathways (i.e., screen-in, keyboard/mouse-out), confirming that capable agents can operate without relying on game-specific APIs \cite{CRADLE, ui-venus}.

\todo{Consider whether to remove the previous paragraph about ``timely''}

Inspired by this clear convergence of market demand and emerging academic feasibility, this project aims to bridge the gap. The core objective is to define and build a prototype for a \textbf{companion-style assistant}. This assistant is envisioned as a persistent, in-game partner that delivers \textbf{event spotting} and \textbf{tactical guidance}, leveraging these new, reproducible methodologies to enhance the player's experience without interrupting gameplay.

\todo{Consider to improve this paragraph after finishing other parts}

% ==========
% 1.2 Key Challenges
% ==========
\subsection{Key Challenges}

In real player scenarios, the challenge is not just the individual model score, but the overall experience that is \textbf{stable, accurate, fast, and controllable}. Below, the challenges and possible solutions are listed based on recent literature:

\paragraph{Long-Horizon Stability.} When using only GUI (GCC), errors and misalignments can accumulate along the interaction chain, leading to the problem of ``drifting off track.'' This can be measured with \textit{pass@k} (firstly pass the test at k-th attempt) and \textit{Invalid\%} (percent of invalid actions) \cite{Benchmarking-VLA-VLM}. The combination of ``planning + skills (macro) + reflection + memory'' can alleviate drift, but it is not a panacea \cite{CRADLE}.

% TODO: æ˜¯å¦è¦è¯´æ˜æ¯ä¸ªæŒ‡æ ‡

\paragraph{Vision-Centric Grounding \& Memory.} In purely visual or continuous space setups, localization/tracking/counting, timing control, and long-term visual memory are current model limitations. These can be statistically measured by \textit{OAS} (opportunity-normalized success), broken down by opportunity types such as ``pickable items/time points/path nodes'' \cite{v-mage}.

\paragraph{Invalid Actions \& Think-Action Mismatch.} Converting free text into actions often results in ``thinking correctly but clicking the wrong place.''  Using the combination of structured output and legal move constraints can reduce invalid predictions, which can be used to evaluate calibration by \textit{Brier MAE}. On the training side, rewards based on four granularities, ``format/type/coordinates/content'', align execution details \cite{Benchmarking-VLA-VLM, ui-venus}. At the implementation level, output strategies such as ``\textit{semantic to allowed actions}'' mapping or \textit{probability modeling over the set of allowed actions} can be used to suppress out-of-bounds and misaligned actions from the start \cite{llm-ga}.

\paragraph{Out-of-Distribution \& Protocol Consistency.} When the environment or version changes, comparison results become difficult. \textit{Procedural generation} is needed for out-of-distribution (OOD) evaluation, with fixed \textit{post-processing} and prompt scaffolding to control variables, and reporting \textit{macro/micro} metrics under a \textit{unified protocol} and \textit{leaderboard/battle arena} \cite{Benchmarking-VLA-VLM, lmgame-bench, ORAK}.

\paragraph{Latency \& User Experience (UX).} An assistant must ``respond instantly.'' The key metrics are \textit{RT} (reaction time per opportunity) and \textit{voice RTT} (round-trip voice latency). Engineering strategies to reduce latency include \textit{quantization/pruning/KV caching/streaming decoding} and \textit{on-device/edge-cloud collaboration}, along with single-flight and barge-in strategies \cite{on-device-llm}.

\paragraph{Safety \& Robustness.} High-risk actions must be ``confirmable, rollbackable, and traceable.'' The approach includes \textit{permission whitelists}, \textit{double confirmation}, \textit{shadow execution}, and \textit{rollback/emergency stop}, with \textit{logging/auditing} to locate \textit{think-action mismatch} \cite{llm-brained-gui, mllm-gui, os-agents}.

% ==========
% 1.3 Scope & Boundaries
% ==========
\subsection{Scope and Boundaries}

To address the issue of inconsistent action interfaces and the challenge of reproducibility caused by reliance on scene-specific APIs, as mentioned in previous sections, this project makes the following clear definition of its technical scope:

\begin{itemize}
    \item \textbf{Unified action interface via GUI (GCC)}: This refers to a human-homomorphic channel with screen-in, keyboard/mouse-out.
    \item \textbf{Structured output as the default path}: The default action generation and execution path is defined as \textbf{structured output + constrained decoding}.
\end{itemize}

This scope is defined based on its demonstrated feasibility. Existing studies have shown that, without relying on application-specific APIs, long-horizon tasks can be completed through a pipeline consisting of "planning, macro/skill organization, self-reflection, and memory". Additionally, on real platforms, the "screenshot to structured action" end-to-end navigation path has been proven to be reproducible, and \textbf{legal move constraints} significantly reduce invalid actions \cite{CRADLE, ui-venus, Benchmarking-VLA-VLM}.

\paragraph{Out-of-scope.}
This project does not involve \textbf{adapting application/game-specific APIs} on a per-application basis. It does not commit to \textbf{large-scale end-to-end training and data collection} in this report. It will not rely on platform-level enhancement privileges (such as A11y/private DOM hooks). \textbf{VLA direct-action} will be used only as a benchmark comparison and not as the default path \cite{Benchmarking-VLA-VLM}.


% ==========
% 1.4 Design Principles & System Preview
% ==========
\subsection{Design Principles \& System Preview}
\todo{Put to Section 3. Project Plan, or just remove?}

\paragraph{Design principles.}
This project follows four principles: \textbf{Unified Input} (ensures portability across applications), \textbf{Structured Output} (reduces invalid actions and is easy to audit), \textbf{Protocol Consistency} (ensures reproducibility and ablative evaluation), \textbf{Low-Coupling Orchestration} (MCP-style, facilitates modular insertion/removal of skills/tools).

\paragraph{System preview.}
The system flow is as follows: \textbf{screen/audio} capture, lightweight \textbf{VLM} perception, \textbf{agentic} (planning/memory/reflection) modules, \textbf{MCP-style} skill/tool routing (including OCR/retrieval/computation \textbf{tool use}), \textbf{GUI execution} (keyboard and mouse), and finally passing through the \textbf{safety} module (confirmation/rollback/emergency stop). To reduce end-to-end latency, the deployment strategy will combine the \textbf{tool-augmented MLLM} approach with \textbf{on-device inference} quantization/caching strategies as key engineering tactics \cite{tool-aug-mllm, on-device-llm}.

\todo{$\uparrow$ Not ready yet}


% ==========
% 1.5 Project Objectives & Expected Deliverables
% ==========
\subsection{Project Objectives \& Expected Deliverables}

\paragraph{Project Objectives.} 
\begin{itemize}
    \item [(i)] Develop a \textbf{GCC-based} \textbf{assistant} real-time prototype that covers event prompting, strategy suggestions, and voice loops. 
    \item [(ii)] Organize \textbf{skills, planning, memory, reflection} using \textbf{MCP-style orchestration}, enabling plug-and-play and auditability without relying on application-specific APIs.
    \item [(iii)] Define a set of \textbf{small and reproducible} evaluation elements (task scripts and metric families), focusing on \textbf{pass@k/TTC/Invalid\%/macro-micro} and \textbf{OAS/RT/APO} experience-related metrics \cite{ORAK, lmgame-bench}.
\end{itemize}

\paragraph{Expected Deliverables.} 
\begin{itemize}
    \item [(a)] \textbf{System Prototype:} Screen capture and lightweight perception, agentic modules, MCP-style skill bus, GUI executor, and basic safety safeguards.
    \item [(b)] \textbf{Evaluation Scripts and Configuration:} Reproducible task scripts, metric calculation, and logging/audit tools (including module switches for comparison).
    \item [(c)] \textbf{User Documentation and Demos:} Installation/operation instructions, configuration templates, and demo videos.
\end{itemize}

% ==========
% 1.6 æœ¯è¯­ä¸èŒƒå›´å¯¹é½ï¼ˆGlossary \& Scope Alignmentï¼‰
% ==========
\subsection{Glossary \& Terminology}

\noindent\todo{Add more \dots}

Since this research area is relatively new, the terminology and naming conventions across different works are not yet unified. Therefore, before entering the literature review, this project aligns key terms and definitions:

\paragraph{GCC (General Computer Control):} A human-homomorphic action interface defined as screen-in + keyboard/mouse-out; this is the default execution channel in this project \cite{llm-brained-gui}.

\paragraph{LAM (Large Action Models):} A family of models where structured actions are treated as first-class outputs; referenced as a comparative paradigm in this project \cite{llm-brained-gui}.

\paragraph{VLM vs VLA:} Text/JSON output mapped into action space vs direct action vectors/distributions; evaluation will consistently use legal move mapping + constrained decoding approach \cite{mllm-gui}.

\paragraph{Scaffold vs Orchestration (MCP-style):} The former refers to the stable interaction "scaffolding" during evaluation, while the latter refers to module/tool registration and routing; both are complementary \cite{os-agents}.

\paragraph{Metric Definitions:} \textbf{pass@k}, \textbf{TTC}, \textbf{Invalid\%}, and \textbf{macro/micro} will be reported together; opportunity-driven \textbf{OAS/RT/APO} serve as core supplements in companion-style scenarios \cite{agent-ai}.

\paragraph{Memoryâ€“Reasoningâ€“I/O (M-R-I/O):} The internal working division and terminology anchor of this project. Here, planning and reflection align with reasoning, skills represent action output forms on the I/O side, and memory remains independent. The output side will default to a ``semantic-to-allowed action'' mapping or a compliance strategy that models probabilities over the set of allowed actions \cite{llm-ga}.

% TODO: æ¨¡å—åŒ–çš„skills/macroï¼Œæ–¹ä¾¿ç¤¾åŒºå¼€å‘é€‚ç”¨äºä¸åŒæ¸¸æˆçš„æ’ä»¶ï¼Ÿâœ…åæœŸå¿…é¡»è¦åŠ 

% TODO: ä¼ ç»™å¤§æ¨¡å‹çš„æ˜¯å¤„ç†åçš„å›¾ç‰‡ï¼Ÿæ¯”å¦‚è¯´ç”¨YOLOæ‰“æ ‡å’Œä½ç½®ï¼ŸğŸ¤”åé¢å†çœ‹