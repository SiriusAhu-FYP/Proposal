\section{Literature Review}
% > 说明：按taxonomy组织；每小节末有“与本文关系”。Cradle放在2.2（GUI/GCC）。涉及了2.1-2.9。

\subsection{Perception: Modalities \& Grounding}
% > 输入模态与定位。涉及2.3-2.4。
\todo{视觉为主（screen/video）+ 可选音频（audio）；VLM能力：检测/描述/grounding；可简单对照VLA（直接产出action tokens）与VLM+tool的差别。
与本文关系：本节仅界定术语与能力范畴。}

\subsection{Action Interfaces: GUI (GCC) \& MCP-style Orchestration}
% > 说明：以GUI（GCC）作为统一执行通道；MCP作为内部“技能总线（skill bus）”的注册/编排思想（不讨论API适配）。Cradle在此作GUI可行性的代表。涉及2.6-2.7。

在动作接口上，\textbf{GUI} 路线以 \textbf{General Computer Control (GCC)} 为统一通道（\emph{screen-in, keyboard/mouse-out}），强调跨应用/跨游戏的可迁移性（portability）与人类同态交互（human-homomorphic interface）。代表性工作 \emph{Cradle} 展示了在不依赖应用专用接口的前提下，通过\emph{规划—技能整理（skill curation/registry）—反思—记忆}的管线完成长链路任务（desktop/games），为 \emph{GUI} 可行性提供了实证支持。与此同时，\textbf{MCP}（Model Context Protocol）提供了\emph{模块注册/编排（module registration/orchestration）}的协议化思路：在不改变输出仍为 \emph{GUI} 的条件下，\emph{skills/macros、planning、memory、reflection} 等可在统一接口下组织，便于可复现实验与消融比较。\cite{tan2024cradle,park2025orak}

\paragraph{Takeaway}
文献显示：\emph{GUI（GCC）}提供统一接口与较低移植门槛；协议化编排（如\emph{MCP}）有助于模块化与复现性。与本文关系：本节作为动作接口背景与术语界定。

…Cradle 以统一的 GUI（GCC）通道展示了从“屏幕输入→内在推理→键鼠控制”的闭环（见 \autoref{fig:cradle-overview}）。

\picHere{./assets/images/from-papers/cradle01.jpg}{0.9\linewidth}
{An overview of the CRADLE framework: CRADLE takes video from the computer screen as input and outputs computer keyboard and mouse control determined through inner reasoning (planning, skill curation, reflection, memory) \cite{tan2024cradle}.}
{fig:cradle-overview}

\paragraph{UI-Venus（screenshot-only）}
端到端 GUI 导航，无需 planner/A11y；强调\emph{截图$\to$结构化动作}的通道在真实平台可达到具有竞争力的结果（如 AndroidWorld \,pass@1 与 ScreenSpot 系列定位）\cite{ui-venus}。与本文关系：作为文献实例表明 screenshot-only 路线的可行性。

\subsection{Agentic Modules: Planning, Memory, Reflection, Skills}
% > 机制视角。涉及2.1, 2.2, 2.4。
\todo{规划（planning）、记忆（memory, 用户偏好/历史）、反思（self-reflection, 纠错/风格一致）、技能库（skills/macros, 原子→复合）。
与本文关系：仅作机制分类与代表性做法的回顾。}

\paragraph{历史对齐与稀疏动作增强}
提出 \emph{Self-Evolving Trajectory History Alignment \& Sparse Action Enhancement}：
用当前模型重写历史“思维—动作”轨迹以对齐风格/细节，并上采样稀疏但关键动作（如 LongPress），以改善长链路一致性与泛化\cite{uivenus_rft}。与本文关系：作为处理长链路与长尾动作的文献做法。

\subsection{Learning Paradigms: Zero-shot, RAG, Finetune, IL/RL, Distillation}
% > 训练与推理范式。涉及2.1, 2.3。
\todo{零样本/提示工程、检索增强（RAG for UI schema/FAQ）、轻量微调（LoRA）、模仿/强化（IL/RL）、蒸馏到小模型。与本文关系：范式综述，不含实现承诺。}

以“\emph{指令化}（instructionalization）”增强 RL 代理的上下文理解是一条代表性路线。\emph{R2-Play} 将多模态游戏指令（MGI）并入 \emph{Decision Transformer}（\emph{DTGI}），并通过超网络（\emph{SHyperGenerator}）在训练任务与未见任务间共享知识；作者报告多模态指令较文本/轨迹单模态在多任务与泛化上更优（动机见 \autoref{fig:r2play-motivation}），MGI 的三段式结构——\emph{game description}、\emph{game trajectory}、\emph{game guidance}（含动作、语言引导及关键元素位置）——给出了指令模板（见 \autoref{fig:r2play-mgi}）\cite{r2play}。与本文关系：作为“指令化/Decision Transformer”方向的文献背景。

\picHere{./assets/images/from-papers/r2-play01.jpg}{0.7\linewidth}
{Imagine an agent learning to play Palworld. (1) The agent exhibits confusion when only relying on textual guidance. (2) The agent is confused when presented with images of a Pal sphere and a Pal. (3) The agent understands how to catch a pet through \emph{multimodal guidance}, which combines textual guidance with images of the Pal sphere and Pal \cite{r2play}.}
{fig:r2play-motivation}

\picHere{./assets/images/from-papers/r2-play02.jpg}{0.9\linewidth}
{An illustrative example of \emph{multimodal game instructions (MGI)}. Each instruction consists of three sections: \emph{game description}, \emph{game trajectory}, and \emph{game guidance} (including action, language guidance, and the position of key elements) \cite{r2play}.}
{fig:r2play-mgi}

\paragraph{RFT（GRPO）与动作粒度奖励}
将奖励拆分为\emph{格式/动作类型/坐标/内容}四部分并加权，以同时度量\emph{结构化输出合规性}与\emph{细粒度定位/文本输入}正确性，作为 GUI 导航中 RL-finetune 的代表做法之一\cite{uivenus_rft}。与本文关系：作为 RL-finetune 在 GUI 导航中的奖励设计范例。

\subsection{Benchmarks \& Datasets (OS-like, Games, Desktop)}
% > 三段法——Orak（统一评测/消融/MCP思想）→ lmgame-Bench（脚手架+污染控制）→ Procedural-generation（OOD方法学）。涉及2.6。每段末“与本文关系”一句，不作采用承诺。

\paragraph{Orak（统一评测/消融/MCP思想）}
\emph{Orak} 通过 \emph{MCP} 实现\emph{plug-and-play} 的代理—环境解耦，并在统一配置下检验
\emph{planning / reflection / memory / skills} 等\emph{agentic modules} 的边际贡献（ablation），配套
\emph{Leaderboard/Battle Arena} 与训练轨迹数据（fine-tuning trajectories），将\emph{机制—性能—配置}一体化呈现\cite{park2025orak}。
与本文关系：作为“统一评测与消融”的代表性基准。

\paragraph{Procedural-generation（OOD方法学）}
基于\emph{procedural generation} 的开放式评测在可控生成下构造\emph{OOD} 与多步任务压力，比较
\emph{VLA/VLM} 在\emph{架构/训练数据/输出后处理}等变量下的泛化与稳健性，并配套工具链以保证\emph{reproducibility}\cite{guruprasad2025benchmarking}。
与本文关系：作为 OOD/变量可控的评测方法学背景。

\paragraph{lmgame-Bench（脚手架与污染控制）}
\emph{lmgame-Bench} 将“\emph{游戏→评测}”系统化：用 \emph{Gym-style} 接口与\emph{perception/memory scaffolds}
稳定\emph{prompt} 并剔除\emph{污染}，在多模型下获得良好分离度，并通过\emph{相关性分析}展示“各游戏探测的能力混合不相同”；另报告单一游戏的 \emph{RL} 训练对\emph{未见游戏}/\emph{外部规划任务}存在迁移\cite{hu2505lmgame}。
与本文关系：作为“脚手架/污染控制/迁移观察”的评测文献。

…为减少提示方差并抑制污染，lmgame-Bench 以模块化脚手架稳定“感知—记忆—推理”的交互回路（见 \autoref{fig:lmgame-harness}）。

\picHere{./assets/images/from-papers/lmgame01.jpg}{0.9\linewidth}
{lmgame-Bench uses modular harnesses—such as perception, memory, and reasoning modules—to systematically extend a model’s game-playing capabilities, enabling iterative interaction loops with a simulated game environment \cite{hu2505lmgame}.}
{fig:lmgame-harness}

\paragraph{V\!-MAGE（vision-centric, visual-only, continuous-space）}
该框架以\textbf{仅视觉输入}与连续空间的游戏环境，评测多模态模型的视觉中心能力，覆盖定位、轨迹追踪、时机、视觉记忆及更高层时序推理；其评测管线支持分离“模型/策略”，并采用 Elo 风格排名进行相对强度比较；作者报告现有模型与人类表现存在差距、常见感知错误与锚定偏差，且有限历史上下文会限制长时规划\cite{v-mage}。
与本文关系：作为视觉中心评测的代表性基准。

\picHere{./assets/images/from-papers/v-mage01.jpg}{0.9\linewidth}
{The overview of the V-MAGE benchmark, designed to evaluate vision-centric capabilities and higher-level reasoning of MLLMs across 5 free-form games with 30+ levels \cite{v-mage}.}
{fig:v-mage-overview}

\subsection{Evaluation Protocols \& Metrics}
% > 强关联本文贡献；此处仅回顾文献做法，不作实现承诺。
\todo{客观：success rate, time-to-completion, no-misclick/rollback rate, latency（voice RTT, frame→hint时间）；主观：advice adoption, user satisfaction。与本文关系：术语与度量背景。}

文献中常将\emph{输出结构化与解码约束（structured output \& constrained decoding）}纳入统一协议：动作以 JSON Schema 与白名单规范，经内部路由后由 GUI 执行；并以\emph{Invalid Action Rate} 作为守门指标。对稀疏/时序敏感技能，亦有工作报告机会归一化成功率（\emph{OAS}）、反应时延（\emph{RT}）与每次机会尝试数（\emph{APO}）等指标\cite{guruprasad2025benchmarking}。
此外，关于\emph{post-processing}（技能解码、重试/回滚）的显式记录亦见于将游戏转化为可靠评测的文献\cite{hu2505lmgame}。
文献中亦有采用 \emph{Elo-style ranking} 进行跨任务相对强度比较的做法\cite{v-mage}。

\subsection{Deployment \& Real-time Considerations}
% > 工程现实。涉及2.2, 2.6。
\todo{本地/云混合、量化（INT4/FP8）、流式解码、语音中断（barge-in）、资源占用与帧率影响。与本文关系：工程背景。}

\subsection{Safety, Permissions \& Robustness}
% > 安全边界。涉及2.2。
\todo{权限模型（whitelist, scope）、操作确认、影子模式（shadow mode）先预测后执行、回滚/急停。与本文关系：安全与鲁棒性背景。}

如 \autoref{fig:uivenus_qual} 所示，think–action 不一致（mismatch）揭示了 MLLM 的“幻觉”（hallucination）风险\cite{ui-venus}。

\picHere{./assets/images/from-papers/ui-venus01.jpg}{0.95\linewidth}
{One trace of UI-Venus on the task named MarkorDeleteAllNotes in AndroidWorld. We can observe that UI-Venus successfully achieves the goal and has the reflection ability in Step 3. However, there also exists the conflict between think and action in Step 5, remaining as a future work about how to solve MLLM’s hallucination.\cite{ui-venus}}
{fig:uivenus_qual}

\subsection{Synthesis: Trends, Gaps \& Our Niche}
% > 综述收束到本文位置；仅做趋势与缺口陈述，末尾一句“与本文关系”定位。
当前趋势是在\emph{GUI（GCC）}通道上引入\emph{协议化/模块化}编排以支撑可复现实验与消融；真实多类型游戏的统一评测（如\emph{Orak}）与\emph{visual-only/continuous-space} 的视觉中心评测（如\emph{V\!-MAGE}）并行发展；把\emph{脚手架/污染控制}引入评测协议（如\emph{lmgame-Bench}）成为常见做法。与本文关系：本文研究定位于\emph{伴随式（companion-style）}场景的文献回顾与术语/评测背景梳理。
