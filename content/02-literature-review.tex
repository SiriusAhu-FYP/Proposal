\section{Literature Review}
This section reviews the foundational research underpinning the development of AI-driven game companions. It first establishes the technical feasibility of using human-homomorphic interfaces for game-agnostic control. Following this, it examines the evaluation protocols and performance metrics critical for benchmarking such systems. The review then explores the core agentic modules—planning, memory, and reflection—necessary for maintaining stability in long-horizon tasks. Subsequently, it details the modern learning paradigms required for robust agent training, before concluding with a discussion of persistent challenges, such as latency and safety, and a synthesis of the key literature gaps this project aims to address.

\subsection{Feasibility of Human-Homomorphic Interfaces}

Achieving the proposed unified action interface requires grounding the agent in \textbf{human-homomorphic interfaces}, a paradigm often referred to as General Computer Control (GCC). This approach uses screen-in, keyboard/mouse-out interaction to bypass the need for game-specific APIs, which is foundational for a truly game-agnostic assistant \cite{CRADLE}. The feasibility of this paradigm, however, rests on solving key challenges in stability, representation, and perception. A primary challenge is ensuring long-term stability, which the CRADLE framework addresses by enforcing \textbf{legal move constraints}. By restricting the agent to a valid-action set and enforcing keyboard/mouse action validity, it significantly reduces error accumulation and demonstrates improved generalization across unseen games \cite{CRADLE}.

Beyond just constraining moves, the representation of both actions and states is a non-trivial factor in cross-game performance. This is corroborated by the ORAK benchmark, which emphasizes that the \textbf{move representation format}—specifically, the methods used for action mapping—directly affects an agent's ability to generalize across diverse game environments \cite{ORAK}. This requirement for clear representation extends to the agent's understanding of the game state. The UI-Venus framework highlights that \textbf{state representation}—specifically, structured output at multiple granularities such as format, type, and coordinates—is crucial for improving reliability when controlling UI elements, leading to more dependable agent behaviour \cite{ui-venus}.

While these works provide a clear path forward for API-free interaction, this "vision-in, action-out" model is not without its perceptual challenges. The V-MAGE benchmark, which is designed to assess vision-centric capabilities, demonstrates that \textbf{vision-only inputs} pose significant generalization challenges for game-playing agents, particularly in handling novel scenes or tracking objects \cite{v-mage}.

\subsection{Evaluation Protocols \& Performance Metrics}

To rigorously measure the performance of AI assistants in complex, dynamic environments, a new generation of evaluation frameworks and specific metrics has been established. Proper benchmarking is essential for systematically evaluating AI performance across diverse game types and tasks, with systems like ORAK and lmgame-bench providing comprehensive frameworks for ensuring consistent and reliable results \cite{ORAK, lmgame-bench}. These frameworks guide the design of scaffolding and feedback mechanisms that enhance AI stability.

Within these evaluations, researchers rely on a suite of metrics to assess an agent's effectiveness, efficiency, and error rate. General metrics such as \texttt{pass@k}, for instance, are commonly used to measure how many attempts an AI needs to successfully complete a task over repeated trials. More specific to GUI agents, metrics like \texttt{Invalid\%} track errors by measuring the proportion of invalid actions taken by the AI. This is a critical diagnostic, as a high \texttt{Invalid\%} indicates the model is struggling to produce valid outputs within the specified action space \cite{Benchmarking-VLA-VLM}.

Beyond simple success or error rates, recent work has introduced more sophisticated systems for comparative and granular assessment. To address the difficulty of comparing performance across different games, V-Mage introduces a \textbf{Dynamic ELO system} to standardize agent performance \cite{v-mage}. The same framework also moves beyond a single success score by using \textbf{"Unit Tests for Core Visual Abilities"}, a method for providing a granular assessment of an agent's specific skills, such as positioning, trajectory tracking, and visual memory \cite{v-mage}.

\subsection{Agentic Modules and Stability in Long-Horizon Tasks}

For an assistant to be a reliable, long-term partner, it must employ a set of \textbf{agentic modules}—such as planning, memory, and reflection—to manage complex tasks and mitigate error accumulation. While single-step actions are relatively simple, maintaining stability over long-horizon tasks requires a robust architecture where these modules can handle complex, sequential decision-making.

The most critical of these modules are \textbf{planning and reflection}, which work in a continuous loop. Planning, often achieved through \textbf{task segmentation}, involves decomposing complex goals into manageable sub-tasks, while reflection allows the agent to self-correct and adapt to evolving situations \cite{os-agents, CRADLE}. Frameworks like CRADLE demonstrate how an agent can adjust its plan iteratively during a task based on self-reflection. This is supported by surveys like OS-Agents, which identify the ability to adapt plans based on \textbf{environmental feedback} as a cornerstone of modern agent design \cite{CRADLE, os-agents}.

This feedback loop is heavily reliant on \textbf{memory}. Both short-term memory (e.g., recent actions) and long-term memory (e.g., learned strategies) are essential for stabilizing performance and preventing task drift. The importance of this module is highlighted by benchmarks like ORAK, which is designed to test long-term memory in adventure games, and lmgame-bench, which explicitly provides \textbf{memory scaffolding} to even allow LLMs to attempt long-horizon tasks \cite{ORAK, lmgame-bench}.

To make long-term planning tractable, agents also rely on a curated set of \textbf{atomic skills}. The CRADLE framework, for example, introduces \textbf{"Skill Curation"} as a core module, where the agent dynamically generates and updates skills that can be recombined to solve complex tasks \cite{CRADLE}. This modularity is complemented by advanced refinement techniques. The UI-Venus report introduces a \textbf{"Self-Evolving Trajectory History Alignment"} framework, which acts as a form of automated reflection. By re-evaluating and filtering its own past "thought-action pairs," the agent refines its historical context, leading to "more coherent planning" and improved performance over time \cite{ui-venus}.

Collectively, these modules of planning, memory, and skill curation are not just for task completion but are essential for \textbf{error mitigation}. The feedback loops described in the OS-Agents survey and the memory scaffolds in lmgame-bench are designed to prevent the agent from losing focus or drifting off-task, ensuring it can adjust its approach in real-time to maintain stability during long sessions \cite{lmgame-bench, os-agents}.

\subsection{Learning Paradigms for Robust Agent Training}

A stable agent architecture must be supported by a robust training paradigm that bridges the gap between single-step imitation and sustained, interactive assistance. While many foundational models are trained via supervised imitation (e.g., behavior cloning), this paradigm struggles with the reward sparsity and long-horizon, multi-step nature of interactive tasks. Consequently, modern agent systems are increasingly applying \textbf{Reinforcement Fine-Tuning (RFT)} on top of pre-trained models. This approach, as demonstrated in works like UI-Venus, AgentGym-RL, and research on autonomous driving, has proven essential for adapting agents to the complex, sequential decision-making required in dynamic environments \cite{ui-venus, agent-gym-rl, rl-fine-tune-driving}.

This shift to RFT is enabled by two key mechanisms: sophisticated reward shaping and iterative refinement loops. To overcome sparse rewards, systems employ a combination of \textbf{dense rewards} (feedback at each timestep) and \textbf{auxiliary rewards} (feedback for sub-goals or correct formatting). This reward structure is shown to accelerate learning and improve task performance, especially in complex, long-horizon scenarios \cite{agent-gym-rl, rl-fine-tune-driving}. This is then combined with iterative \textbf{feedback loops} that allow the agent to refine its strategy over time. Examples include the "progressive interaction scaling" proposed by AgentGym-RL, or the "self-evolving trajectories" of UI-Venus, both of which are methods for the agent to correct mistakes and adapt its policy \cite{agent-gym-rl, ui-venus}.

Finally, a critical component for reliability, especially in GUI environments, is the use of \textbf{structured action generation}. To reduce errors and constrain the vast output space of a large language model, agents are trained to generate actions in a highly structured form, such as JSON or discrete macro actions, rather than as free-form text. The UI-Venus framework, for example, relies on this approach to ensure that generated actions are valid and reliable, which is a crucial step in preventing model-induced errors during execution \cite{ui-venus}.

\subsection{Challenges and Future Directions}

Despite the rapid progress in agent capabilities, several persistent challenges must be addressed to develop truly robust, real-time AI companions. The most immediate of these is \textbf{latency}, which is critical for a seamless user experience in interactive entertainment. Research into on-device processing, such as the UI-Venus framework, aims to solve the "unacceptably long inference latency" of large models \cite{ui-venus}. This is a non-trivial requirement; studies in real-time human-AI coordination have empirically demonstrated that latency beyond approximately 100ms is perceived as lag and significantly degrades the sense of fluid cooperation \cite{liu2023llm}.

Beyond performance, significant questions of \textbf{safety and robustness} remain. For an agent to be trusted with any level of control, it must be able to handle dangerous actions and critical errors. The OS-Agents survey identifies "Error Recovery Mechanisms" (such as rollbacks) and "Human-in-the-Loop Control" (such as confirmation mechanisms) as major unsolved challenges for the field \cite{os-agents}. This need for alignment is a key research area, focusing on robustifying agents to ensure safe "transfer" to new situations and prevent unintended consequences \cite{zubia2024robustifying}.

A primary cause of such errors is the agent's difficulty with \textbf{out-of-distribution (OOD)} generalization. Both V-Mage and the Benchmarking-VLA-VLM paper identify OOD generalization as a critical failure point \cite{v-mage, Benchmarking-VLA-VLM}. V-Mage, for example, highlights that vision-only models struggle significantly to adapt to new visual scenarios \cite{v-mage}. This finding is not limited to vision; the Benchmarking-VLA-VLM study found that all evaluated models (VLAs and VLMs) had "significant limitations in zero-shot generalization to OOD tasks," noting that their performance was heavily influenced by factors like action representation and prompt engineering \cite{Benchmarking-VLA-VLM}.

These challenges collectively define the \textbf{future directions} for the field. As outlined in the CRADLE framework, research must prioritize enhancing an agent's ability to handle complex, multi-step tasks while ensuring real-time reliability. This requires enhancing \textbf{multi-modal capabilities}, improving accuracy in fine-grained control, and solving the "prohibitive inference latency" of current systems, all while ensuring agents remain resilient in unseen environments \cite{CRADLE}.

\subsection{Synthesis and Gaps in the Literature}

This review reveals that while significant progress has been made in functional agent control, a clear and actionable gap exists in the literature regarding unified, companion-style assistants. Current research is largely bifurcated, prioritizing either high-performance task completion or the fundamentals of relational AI, with a significant lack of academic frameworks that effectively merge the two. For instance, while AI systems show growing competence in task execution, recent work such as the \textbf{Emotion AWARE} framework highlights that integrating "multi-granular and explainable" \textbf{emotional intelligence} remains a complex, unsolved challenge—a feature largely absent in current agent architectures \cite{gamage2024emotion}. This gap is evident in state-of-the-art functional systems; frameworks like UI-Venus have made strides in performance via RFT and structured actions, but their architecture is exclusively focused on functional task completion and does not address this relational component \cite{ui-venus}.

This project is therefore positioned to fill this gap by creating an assistant that architecturally blends both companionship and agency. In doing so, it will also address two other persistent challenges in the field: \textbf{real-time adaptability} and \textbf{cross-game transferability}. The former remains a recognized challenge, with benchmarks such as lmgame-bench being specifically designed to test the limitations of current models in spatiotemporal and long-context reasoning \cite{lmgame-bench}. The latter, the difficulty of creating \textbf{game-agnostic agents}, has been pointed out by numerous studies. Both the ORAK benchmark, which highlights the difficulty of evaluating agents across diverse games, and the CRADLE framework, which was built to solve the reliance on proprietary game APIs, confirm that cross-game transferability is a key hurdle for the field \cite{CRADLE, ORAK}.

By focusing on a human-homomorphic interface and feedback-driven learning, this project will directly contribute to these unsolved areas, demonstrating a system capable of the deeper, functional partnership that current literature lacks.