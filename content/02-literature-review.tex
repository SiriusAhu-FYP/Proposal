\section{Literature Review}
This section reviews the foundational research on AI agents for complex, interactive tasks, which underpins the development of the proposed game companion. It first establishes the technical feasibility of using human-homomorphic interfaces for game-agnostic control. Following this, it examines the evaluation protocols and performance metrics critical for benchmarking such systems. The review then explores the core \textbf{agentic chain}—a synthesis from recent literature comprising \textbf{planning, skills, reflection, and memory}—necessary for maintaining stability in long-horizon tasks. Subsequently, it details the modern learning paradigms required for robust agent training, before concluding with a discussion of persistent challenges, such as latency and safety, and a synthesis of the key literature gaps this project aims to address.

\subsection{Feasibility of Human-Homomorphic Interfaces}

Achieving the proposed unified action interface requires grounding the agent in \textbf{human-homomorphic interfaces}, a paradigm often referred to as General Computer Control (GCC). The feasibility of this approach is strongly demonstrated by Tan et al. in the \textbf{CRADLE} framework, which introduces the ``Screen as input, keyboard/mouse as output'' paradigm. This proves that a unified agent can perform complex, long-horizon tasks across different software, including games, without relying on specialized APIs \cite{CRADLE}. This pipeline, proceeding from \textbf{screenshot to structured action}, is further validated by Gu et al. in the \textbf{UI-Venus} framework, who use Reinforcement Fine-Tuning (RFT) to successfully train agents on GUI tasks \cite{ui-venus}. This is also supported by Wang et al. (2025) in the \textbf{Ponder \& Press} framework, who argue that splitting GUI tasks into ``Ponder'' (planning) and ``Press'' (grounding) improves reliability \cite{ponder-press}.

A key component of this paradigm is advanced planning. The \textbf{Agent S} framework, introduced by Agashe et al.~(2024), provides another strong validation of the GCC paradigm for complex tasks. Its key contribution is ``Experience-Augmented Hierarchical Planning,'' where the agent learns from both external knowledge and its own internal ``narrative'' and ``episodic'' memory to decompose and solve long-horizon goals \cite{AgentS}.

While feasible, this approach requires mechanisms to ensure stability. The \textbf{ORAK} benchmark, introduced by Park et al. (2025), highlights the importance of using a restricted valid-action set, or \textbf{Legal Move Constraints}. This feature, which greatly constrains the model's output space, is a critical optimization that can reduce action errors by 15-20\% \cite{ORAK}. Representation of both state and action is also critical. The ORAK benchmark emphasizes that standardized \textbf{State Representation Formats} (e.g., agent position, task progress) are essential for providing rich context \cite{ORAK}. For \textbf{Move Representation Formats}, the \textbf{CodeAct} framework from Liang et al. (2024) proposes a powerful solution by using \textbf{executable Python code} as the agent's output. This is more robust than static JSON, as it can handle complex logic and allows the agent to \textbf{self-debug} by receiving and correcting its own error tracebacks \cite{CodeAct}.

On the input side, the ORAK benchmark is explicitly designed to evaluate agents using three different \textbf{input modalities}: Text-only, Image-only, and Text + Image \cite{ORAK}. This is a critical area of research, as the \textbf{V-MAGE} benchmark from Zheng et al. (2025) demonstrates that \textbf{vision-only inputs} pose significant generalization challenges \cite{v-mage}, though other work supports a vision-only (no DOM/HTML) approach for improving generalizability \cite{ponder-press}.

\subsection{Evaluation Protocols \& Performance Metrics}

To rigorously measure the performance of these agents, a new generation of evaluation frameworks has been established. Proper benchmarking is essential, with systems like ORAK and \textbf{LMGame-Bench} providing comprehensive frameworks for ensuring reliable results across diverse game types \cite{ORAK, lmgame-bench}. A primary outcome from these evaluations is the poor performance of current models on out-of-distribution (OOD) tasks. The study by Guruprasad et al. (2025) notes this as poor OOD contextual generalization \cite{Benchmarking-VLA-VLM}, a finding confirmed by other benchmarks and surveys \cite{v-mage, VisualAgentBench, gui-agents-survey}.

Within these evaluations, researchers rely on a suite of metrics. General metrics such as \texttt{pass@k} are commonly used to measure success over repeated trials. More specific to GUI agents, metrics like \texttt{Invalid\%} track the proportion of invalid actions, a critical diagnostic for models struggling to produce valid outputs \cite{Benchmarking-VLA-VLM, gui-agents-survey}. To properly evaluate agents on long-horizon tasks, benchmarks like ORAK also employ a mix of reward types, including \textbf{Dense Rewards} (for continuous feedback) and \textbf{Auxiliary Rewards} (e.g., for ``correct format output'') to guide and assess complex behaviors \cite{ORAK}.

Beyond simple success rates, recent work has introduced more sophisticated systems for comparative and granular assessment. To address the difficulty of comparing models across different games, Zheng et al. introduce a \textbf{Dynamic ELO system} to standardize agent performance \cite{v-mage}. The same framework also moves beyond a single score by using ``Unit Tests for Core Visual Abilities.'' This method provides a granular assessment of MLLM failures, identifying specific failure modes such as errors in \textbf{positioning}, \textbf{direction}, \textbf{identification} (e.g., seeing paths as obstacles), and a \textbf{conflict} between correct reasoning and incorrect execution \cite{v-mage}. Finally, recent papers argue for a \textbf{Human-Centered Evaluation Framework,} stating that metrics for a practical agent must also include \textbf{latency, safety, privacy, robustness, and error-recovery mechanisms} (like rollbacks) \cite{human-centered-eval}.

\subsection{Agentic Modules and Stability in Long-Horizon Tasks}

For an assistant to be a reliable, long-term partner, it must employ a set of \textbf{agentic modules} to manage complex tasks and mitigate error accumulation. Recent literature, including surveys by Hu et al. (2024) and Xu et al. (2024), has synthesized this into an ``agentic chain'' of planning, skills, reflection, and memory \cite{llm-ga, gp-agents}. The core architecture for this is a hierarchical multi-agent framework, as proposed by Zhang et al. (2025) in \textbf{AgentOrchestra}. This ``conductor'' model features a central ``planning agent'' that decomposes complex objectives and delegates sub-tasks to ``specialized agents'' \cite{AgentOrchestra, Multi-Agent-Collab-Survey}. For this ``plug-and-play'' architecture to function, a standardized ``Model Context Protocol'' (MCP) is necessary, which ORAK and other frameworks use to allow the central conductor to ``seamlessly interact'' with the specialized agentic modules.

The ``conductor'' agent's internal logic is an \textbf{agentic chain}, as demonstrated in the CRADLE framework, which includes modules for ``Info Gathering,'' ``Self-Reflection,'' ``Skill Curation,'' and ``Action Planning'' \cite{CRADLE}. \textbf{Memory} is a critical module in this chain. The Agent S framework provides a powerful model for this, using ``Experience-Augmented Hierarchical Planning'' that retrieves from long-term ``narrative memory'' (past tasks) and ``episodic memory'' (past steps) \cite{AgentS, lmgame-bench, os-agents}. To make long-term planning tractable, agents also rely on a curated set of \textbf{atomic skills} (or ``Macros'') \cite{agent-ai}, which the CRADLE framework implements with its ``Skill Curation'' module \cite{CRADLE}.

To prevent error accumulation and \textbf{task drift}, a dedicated safety module based on the \textbf{BacktrackAgent} framework from Yuan et al. (2025) is required. This module uses a ``Verifier'' (for simple failures), a ``Judger'' (to identify semantic errors, e.g., ``Game Over'' screen), and a ``Reflector'' (to backtrack to a known good state and replan) \cite{BacktrackAgent, lmgame-bench, os-agents}. Finally, a key new direction is \textbf{Self-Evolution}, which the UI-Venus framework implements with its ``Self-Evolving Trajectory History Alignment.'' This method acts as automated reflection, allowing the agent to refine its own reasoning and ``thought-action pairs'' for more ``coherent planning'' \cite{ui-venus, llm-ga}.

\subsection{Learning Paradigms for Robust Agent Training}

A stable agent architecture must be supported by a robust training paradigm. While many foundational models are trained via supervised imitation, modern agent systems are increasingly applying \textbf{Reinforcement Fine-Tuning (RFT)}. The \textbf{UI-AGILE} framework from Lian et al. (2025), for example, provides a SOTA training methodology, proposing novel reward-shaping techniques like a ``Continuous Grounding Reward'' (to incentivize precision) and a ``Simple Thinking'' Reward (to balance speed and reasoning) \cite{UI-AGILE, ui-venus, agent-gym-rl}. To overcome sparse rewards, systems also employ a combination of \textbf{dense rewards} (feedback at each timestep) and \textbf{auxiliary rewards} (feedback for sub-goals or correct formatting) \cite{agent-gym-rl, rl-fine-tune-driving}.

As an alternative to online RFT, \textbf{offline learning paradigms} like the \textbf{Decision Transformer} (DT) (Chen et al., 2021) frame RL as a sequence modeling problem, allowing agents to learn from static datasets of expert behavior, as explored by frameworks like \texttt{R2-Play} \cite{r2-play}. A key paradigm for solving latency is the \textbf{Hybrid Offline Generation} approach from the \textbf{PORTAL} framework (Wang et al., 2025). It uses the LLM \emph{offline} to generate a game-playing policy as a \textbf{Behavior Tree (BT)}, which is then executed at \textbf{zero-latency} by a simple runtime interpreter \cite{PORTAL}.

Finally, reliability is enhanced through \textbf{structured action generation}. The CodeAct framework proposes a highly robust method: using \textbf{executable Python code} as the action output. This allows the agent to \textbf{self-debug} by receiving an error traceback from the interpreter and correcting its own code \cite{CodeAct, We-Need-Structured-Output}. This is often combined with \textbf{Grounded task execution}, which explicitly ties an agent's actions to the UI or game state to reduce ambiguity \cite{agent-ai}.

\subsection{Challenges and Future Directions}

Despite rapid progress, several persistent challenges must be addressed. The most immediate is \textbf{latency}, as studies in real-time human-AI coordination by Liu et al. (2023) have empirically demonstrated that latency beyond approximately 100ms degrades fluid cooperation \cite{liu2023llm}. The PORTAL framework's ``zero-latency'' compiled Behavior Tree approach is a key proposed solution to this \cite{PORTAL, human-centered-eval}.

Second, \textbf{safety and robustness} are critical \cite{os-agents, zubia2024robustifying, human-centered-eval}. The BacktrackAgent framework provides a concrete solution, with a 3-module architecture (``Verifier,'' ``Judger,'' ``Reflector'') to detect, assess, and recover from semantic errors \cite{BacktrackAgent}. This is vital because \textbf{out-of-distribution (OOD) generalization} is a critical failure point for most agents \cite{v-mage, Benchmarking-VLA-VLM, VisualAgentBench}. The PORTAL framework offers a breakthrough solution here as well, demonstrating unprecedented cross-game generalization by compiling policies that were successfully applied to thousands of different 3D games \cite{PORTAL}.

Recent surveys also identify that current agents still struggle with \textbf{long-horizon tasks suffering from error accumulation} and \textbf{exhibiting low fine-grained precision} \cite{mllm-gui, os-agents, gui-agents-survey}. These challenges collectively define the \textbf{future directions} for the field: enhancing \textbf{multi-modal capabilities}, improving \textbf{accuracy} in fine-grained control, and solving the ``prohibitive inference latency'' of current systems \cite{CRADLE, BacktrackAgent, PORTAL}.

\subsection{Synthesis and Gaps in the Literature}

This review reveals that while significant progress has been made in functional agent control, a clear and actionable gap exists in the literature regarding unified, companion-style assistants. The primary literature gap is identified by the \textbf{Four-Quadrant Taxonomy} (Sun \& Wu, 2025), which separates AI companions on an \textbf{(Emotional vs. Functional)} axis. Current research is bifurcated, leaving the blend of these quadrants as a key gap, with frameworks like \texttt{EmotionAWARE} highlighting the complexity of the ``Emotional'' component \cite{Four-Quadrant-Taxonomy, EmotionAWARE}.

This gap is evident in SOTA functional architectures like AgentOrchestra and UI-Venus, which are exclusively focused on task completion (the ``Functional'' axis) and do not include or address the relational component \cite{AgentOrchestra, ui-venus}. The HCI study, \textbf{Cooperation Between Player and AI} (Lundstr{\"o}m, John, 2024), suggests that a functional agent with higher autonomy is key to fostering a strong relational bond, as players tend to reject companions that increase their workload \cite{Cooperation-Player-AI}.

This project is therefore positioned to fill this gap. In doing so, it will also address two other persistent challenges: \textbf{real-time adaptability}, a known gap that benchmarks like LMGame-Bench are designed to test \cite{lmgame-bench}, and \textbf{cross-game transferability}. The difficulty of creating \textbf{game-agnostic agents} is a key hurdle \cite{CRADLE, ORAK}, and the PORTAL framework's success in this area highlights it as a critical SOTA research direction \cite{PORTAL}. By focusing on a human-homomorphic interface and feedback-driven learning, this project will directly contribute to these unsolved areas, demonstrating a system capable of the deeper, functional partnership that current literature lacks.