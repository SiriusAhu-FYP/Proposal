\section{Literature Review}
% > 说明：按taxonomy组织；每小节末有“与本文关系”。Cradle放在2.2（GUI/GCC）。涉及了2.1-2.9。

\subsection{Perception: Modalities \& Grounding}
% > 输入模态与定位。涉及2.3-2.4。
\todo{视觉为主（screen/video）+ 可选音频（audio）；VLM能力：检测/描述/grounding；可简单对照VLA（直接产出action tokens）与VLM+tool的差别。
与本文：选轻量VLM，优先本地（on-device）与流式ASR/TTS。}

\subsection{Action Interfaces: GUI (GCC) \& MCP-style Orchestration}
% > 说明：以GUI（GCC）作为统一执行通道；MCP作为内部“技能总线（skill bus）”的注册/编排思想（不讨论API适配）。Cradle在此作GUI可行性的代表。涉及2.6-2.7。

在动作接口上，\textbf{GUI} 路线以 \textbf{General Computer Control (GCC)} 为统一通道（\emph{screen-in, keyboard/mouse-out}），强调对不同应用/游戏的可迁移性（portability）与统一的人类同态交互（human-homomorphic interface）。代表性工作 \emph{Cradle} 显示出在不依赖应用专用接口的前提下，仍可通过\emph{规划—技能整理（skill curation/registry）—反思—记忆}的管线完成长链路任务（desktop/games），从而为\emph{GUI} 的可行性提供了强证据（evidence）。与此同时，\textbf{MCP}（Model Context Protocol）为\emph{模块注册/编排（module registration/orchestration）}提供协议化思路：在不改变输出仍为\emph{GUI}的前提下，可将\emph{skills/macros、planning、memory、reflection}等以统一接口组织起来，便于系统性消融与复用（plug-and-play）。与本文：我们采用\emph{GUI（GCC）}作为唯一执行通道，并借鉴\emph{MCP} 的注册/编排思想作为内部“技能总线（skill bus）”，统一路由与调用\emph{skills/macros} 等模块。\cite{tan2024cradle,park2025orak}

\paragraph{Takeaway}
基于文献可见：\emph{GUI（GCC）}为跨应用/跨游戏提供了统一接口与较低的移植门槛；协议化编排（如\emph{MCP}）可在\emph{不更改GUI输出}的条件下提升模块化与可复现性。与本文：坚持\emph{GUI}输出，内部采用\emph{MCP-style} 编排以获得结构化、可消融的系统形态。

...Cradle 以统一的 GUI（GCC）通道展示了从“屏幕输入→内在推理→键鼠控制”的闭环，可作为 GUI 可行性的强证据（见 \autoref{fig:cradle-overview}）。

\picHere{./assets/images/from-papers/cradle01.jpg}{0.9\linewidth}
{An overview of the CRADLE framework: CRADLE takes video from the computer screen as input and outputs computer keyboard and mouse control determined through inner reasoning (planning, skill curation, reflection, memory) \cite{tan2024cradle}.}
{fig:cradle-overview}

\paragraph{UI-Venus（screenshot-only）}
端到端GUI导航，无需planner/A11y，强调\emph{截图$\to$结构化动作}的通道仍能在真实平台达SOTA（AndroidWorld\;65.9\%\,pass@1），对我们“GUI优先、统一动作模式”的落地具有直接支撑\cite{ui-venus}.


\subsection{Agentic Modules: Planning, Memory, Reflection, Skills}
% > 机制视角。涉及2.1, 2.2, 2.4。
\todo{规划（planning）、记忆（memory, 用户偏好/历史）、反思（self-reflection, 纠错/风格一致）、技能库（skills/macros, 原子→复合）。
与本文：直接采纳skills+reflection+memory组合，并通过“技能总线”统一管理。}

\paragraph{历史对齐与稀疏动作增强}
提出\emph{Self-Evolving Trajectory History Alignment \& Sparse Action Enhancement}：
用当前模型重写历史“思维—动作”轨迹以对齐风格/细节，并上采样稀疏但关键动作（如LongPress），
改善长链路一致性与泛化\cite{uivenus_rft}.


\subsection{Learning Paradigms: Zero-shot, RAG, Finetune, IL/RL, Distillation}
% > 训练与推理范式。涉及2.1, 2.3。
% > 说明：加入“instructionalization（指令化）/Decision Transformer”子段（放R2-Play）；强调其为数据/提示工程启发，不改变GUI执行。涉及2.1, 2.3。
\todo{零样本/提示工程、检索增强（RAG for UI schema/FAQ）、轻量微调（LoRA）、模仿/强化（IL/RL）、蒸馏到小模型。
与本文：优先零样本+RAG，必要时小规模LoRA以稳UI。}

以“\emph{指令化}（instructionalization）”增强 RL 代理的上下文理解是一条有效路径。\emph{R2-Play} 将多模态游戏指令（MGI）并入 \emph{Decision Transformer}，形成 \emph{DTGI}，并通过超网络（\emph{SHyperGenerator}）在训练任务与未见任务间共享知识；作者报告多模态指令相较于文本或轨迹单模态在多任务与泛化上更优（动机见 \autoref{fig:r2play-motivation}）。MGI 的三段式结构——\emph{game description}、\emph{game trajectory} 与 \emph{game guidance}（含动作、语言引导及关键元素位置）——为“指令卡”提供了清晰模板（见 \autoref{fig:r2play-mgi}）。对我们而言，这更像“数据与提示工程”的启发：为 \emph{GUI} 技能准备轻量的“多模态说明卡”（示例帧+要点文本），在 \emph{MCP-style} 的内部技能总线下统一注册与调用，输出仍保持 \emph{GUI}（kb/mouse）\cite{r2play}.



% —— 2.4 Learning Paradigms 子段：R2-Play 动机图（Palworld） ——
\picHere{./assets/images/from-papers/r2-play01.jpg}{0.7\linewidth}
{Imagine an agent learning to play Palworld. (1) The agent exhibits confusion when only relying on textual guidance. (2) The agent is confused when presented with images of a Pal sphere and a Pal. (3) The agent understands how to catch a pet through \emph{multimodal guidance}, which combines textual guidance with images of the Pal sphere and Pal \cite{r2play}.}
{fig:r2play-motivation}

% —— 2.4 Learning Paradigms 子段：MGI 结构示例 ——
\picHere{./assets/images/from-papers/r2-play02.jpg}{0.9\linewidth}
{An illustrative example of \emph{multimodal game instructions (MGI)}. Each instruction consists of three sections: \emph{game description}, \emph{game trajectory}, and \emph{game guidance} (including action, language guidance, and the position of key elements) \cite{r2play}.}
{fig:r2play-mgi}

\paragraph{RFT（GRPO）与动作粒度奖励}
将奖励拆分为\emph{格式/动作类型/坐标/内容}四部分并加权，既鼓励\emph{结构化输出合规}，
又提升\emph{细粒度定位/文本输入}正确性，是近期GUI导航RL-finetune的代表路径\cite{uivenus_rft}.


\subsection{Benchmarks \& Datasets (OS-like, Games, Desktop)}
% > 说明：三段法——Orak（统一评测/消融/MCP思想）→ lmgame-Bench（脚手架+污染控制）→ Procedural-generation（OOD方法学）。段末各合并一次引用。涉及2.6。
以真实多类型游戏为对象的统一评测框架正在兴起：\emph{Orak} 通过 \emph{MCP} 实现\emph{plug-and-play} 的代理—环境解耦，并在统一配置下检验 \emph{planning / reflection / memory / skills} 等\emph{agentic modules} 的边际贡献（ablation），配套 \emph{Leaderboard/Battle Arena} 的对比体例与用于训练的轨迹数据（fine-tuning trajectories）。这类框架的价值在于：\emph{机制—性能—配置}三者被一体化呈现，既利于横向（模型/模态）又利于纵向（策略/模块）比较。与本文：我们借鉴其“统一评测 + 消融”的组织方式，但将\emph{输出统一为GUI}，并用\emph{MCP-style} 在内部完成技能与模块的注册/编排。\cite{park2025orak}

% > 说明：该文提供“程序生成→开放式→统一对比”的方法学参考；与GUI并不冲突。
与以真实多类型游戏构建统一评测与消融的框架相互补充，一条重要的发展脉络是基于\emph{procedural generation}的开放式评测：在可控生成下构造\emph{OOD}与多步任务压力，统一比较\emph{VLA/VLM}在\emph{架构/训练数据/输出后处理}等变量下的泛化与稳健性，并配套可复用的工具链以保证\emph{reproducibility}。我们采纳其“\emph{协议一致、变量可控}”的组织方式，但将动作执行统一为\emph{GUI}，并用\emph{MCP-style}在内部编排\emph{skills/macros}与\emph{agentic modules}以适配我们的场景需求\cite{guruprasad2025benchmarking}.

% Lit. Review: Benchmarks & Datasets（与 Orak 并列）
与面向真实多类型游戏的统一评测/消融框架互补，\emph{lmgame-Bench} 将“\emph{游戏→评测}”系统化：用 \emph{Gym-style} 接口与\emph{perception/memory scaffolds} 稳定\emph{prompt}、剔除\emph{污染}，在多模型下获得良好分离度，并通过\emph{相关性分析（correlation analysis）}展示“各游戏探测的能力混合并不相同”；进一步，单一游戏的 \emph{RL} 训练对\emph{未见游戏}与\emph{外部规划任务}出现迁移（transfer）。本文沿用其“\emph{协议一致、变量可控}”的评测方法学，但\textbf{执行端统一为 GUI}，并以 \emph{MCP-style} 在内部编排 \emph{skills/macros} 与 \emph{planning/memory/reflection} 以做可复现实验\cite{hu2505lmgame}.

……为减少提示方差并抑制污染，lmgame-Bench 以模块化脚手架稳定“感知—记忆—推理”的交互回路（见 \autoref{fig:lmgame-harness}），从而把“游戏→可靠评测”落到可复现协议之中。

\picHere{./assets/images/from-papers/lmgame01.jpg}{0.9\linewidth}
{lmgame-Bench uses modular harnesses—such as perception, memory, and reasoning modules—to systematically extend a model’s game-playing capabilities, enabling iterative interaction loops with a simulated game environment \cite{hu2505lmgame}.}
{fig:lmgame-harness}

\paragraph{Benchmarks}
在线：AndroidWorld（实时多步交互）；离线：AndroidControl、GUI-Odyssey
— UI-Venus在三者上报告系统对比与SOTA/可比结果\cite{uivenus_rft}.

\paragraph{V\!-MAGE（vision-centric, visual-only, continuous-space）}
该框架以\textbf{仅视觉输入}与连续空间的游戏环境，聚焦评测多模态模型的视觉中心能力，
覆盖定位、轨迹追踪、时机、视觉记忆及更高层时序推理等要素；其评测管线支持分离“模型/策略”，
并采用 Elo 风格排名进行相对强度比较。作者报告现有模型与人类表现存在差距，常见感知错误与锚定偏差，
且有限历史上下文会限制长时规划\cite{v-mage}.

\picHere{./assets/images/from-papers/v-mage01.jpg}{0.9\linewidth}{The overview of the V-MAGE benchmark, designed to evaluate vision-centric capabilities and higher-level reasoning of MLLMs across 5 free-form games with 30+ levels. V-MAGE assesses critical abilities in visual reasoning, providing a comprehensive evaluation of model performance in complex, dynamic environments.}{fig:v-mage-overview}

\subsection{Evaluation Protocols \& Metrics}
% > 强关联本文贡献。涉及2.2, 2.7。
\todo{客观：success rate, time-to-completion, no-misclick/rollback rate, latency（voice RTT, frame→hint时间）；主观：advice adoption, user satisfaction。
与本文：将advice adoption与macro success设为核心指标，配套延迟与稳定性度量。}

% > 说明：把“输出后处理（post-processing）”归入可控变量，便于可复现实验。
为避免实现细节带来的不可比性，我们将输出结构化与解码约束（structured output \& constrained decoding）纳入统一协议：动作以 JSON Schema 与白名单规范，经 MCP-style skill bus 路由后由 GUI 执行；并以 Invalid Action Rate 作为守门指标（目标近零）。在效果度量上，我们采用跨技能均值（macro-averaged success/recall）为主视角，辅以 micro 指标；对稀疏/时序敏感技能，额外报告机会归一化成功率（OAS）、\textbf{反应时延（RT）与每次机会尝试数（APO）}以刻画稳定性与可用性。\cite{guruprasad2025benchmarking}

我们将“\emph{输出后处理（post-processing）}”（如技能解码、重试/回滚策略）显式纳入可控变量，配合\emph{稳定 prompt 与污染控制}的协议，减少实现细节对可比性的干扰\cite{hu2505lmgame}.

文献中亦有采用 \emph{Elo-style ranking} 进行跨任务相对强度比较的做法，V\!-MAGE 为其中一例\cite{v-mage}.


\subsection{Deployment \& Real-time Considerations}
% > 工程现实。涉及2.2, 2.6。
\todo{本地/云混合、量化（INT4/FP8）、流式解码、语音中断（barge-in）、资源占用与帧率影响。
与本文：给出延迟预算（如 $\leq 500$ms 提示、$\leq 1.5$s 语音回路）。}

\subsection{Safety, Permissions \& Robustness}
% > 安全边界。涉及2.2。
\todo{权限模型（whitelist, scope）、操作确认、影子模式（shadow mode）先预测后执行、回滚/急停。
与本文：作为系统必要模块并与“技能总线”联动。}

如 \autoref{fig:uivenus_qual} 所示，think–action 不一致（mismatch）揭示了 MLLM 的“幻觉”（hallucination）风险。\cite{ui-venus}

\picHere{./assets/images/from-papers/ui-venus01.jpg}{0.95\linewidth}
{One trace of UI-Venus on the task named MarkorDeleteAllNotes in AndroidWorld. We can observe that UI-Venus successfully achieves the goal and has the reflection ability in Step 3. However, there also exists the conflict between think and action in Step 5, remaining as a future work about how to solve MLLM’s hallucination.\cite{ui-venus}}
{fig:uivenus_qual}

\subsection{Synthesis: Trends, Gaps \& Our Niche}
% > 综述收束到本文位置。关联全篇。
当前趋势是在\emph{GUI（GCC）}的统一通道上引入\emph{协议化/模块化}的编排（如\emph{MCP-style}），以便做机制消融与可复现实验；真实多类型游戏的统一评测（如\emph{Orak}）正在成为共识。缺口在于：\emph{实时伴随式（companion-style）}场景仍缺乏围绕语音互动与低延迟体验的专门指标与协议。与本文：我们将以\emph{GUI}为唯一执行通道，结合\emph{MCP-style} 编排与伴随式指标，给出可复现的小型协议与演示设置。
