\section{Literature Review}
This section reviews the foundational research on AI agents for complex, interactive tasks, which underpins the development of the proposed game companion. It first establishes the technical feasibility of using human-homomorphic interfaces for game-agnostic control. Following this, it examines the evaluation protocols and performance metrics critical for benchmarking such systems. The review then explores the core \textbf{agentic chain}—a synthesis from recent literature comprising \textbf{planning, skills, reflection, and memory}—necessary for maintaining stability in long-horizon tasks. Subsequently, it details the modern learning paradigms required for robust agent training, before concluding with a discussion of persistent challenges, such as latency and safety, and a synthesis of the key literature gaps this project aims to address.

\subsection{Feasibility of Human-Homomorphic Interfaces}

Achieving the proposed unified action interface requires grounding the agent in \textbf{human-homomorphic interfaces}, a paradigm often referred to as General Computer Control (GCC). The feasibility of this approach is strongly demonstrated by the CRADLE framework, which introduces the "Screen as input, keyboard/mouse as output" paradigm. This proves that a unified agent can perform complex, long-horizon tasks across different software, including games, without relying on specialized APIs \cite{CRADLE}. This "Screenshot → Structured Action" pipeline is further validated by the UI-Venus framework, which uses Reinforcement Fine-Tuning (RFT) to successfully train agents on GUI tasks, reinforcing that GUI-based control is a viable and powerful paradigm \cite{ui-venus}.

While feasible, this approach requires mechanisms to ensure stability. The ORAK benchmark highlights the importance of using a restricted valid-action set, or \textbf{Legal Move Constraints}. This feature, which greatly constrains the model's output space, is a critical optimization that can reduce action errors by 15-20\% \cite{ORAK}. The same benchmark also emphasizes that standardized \textbf{Move Representation Formats} and \textbf{State Representation Formats} are non-trivial and essential for cross-game generalization. This includes standardizing how actions are defined and what contextual state (e.g., agent position, task progress) is provided to the agent \cite{ORAK}.

On the input side, the ORAK benchmark is explicitly designed to evaluate agents using three different \textbf{input modalities}: Text-only, Image-only, and Text + Image, allowing for direct comparison \cite{ORAK}. This is a critical area of research, as the V-MAGE benchmark demonstrates that \textbf{vision-only inputs} pose significant generalization challenges for game-playing agents, particularly in handling novel scenes or tracking objects \cite{v-mage}.

\subsection{Evaluation Protocols \& Performance Metrics}

To rigorously measure the performance of these agents, a new generation of evaluation frameworks has been established. Proper benchmarking is essential, with systems like \textbf{ORAK} and \textbf{lmgame-bench} providing comprehensive frameworks for ensuring reliable results across diverse game types \cite{ORAK, lmgame-bench}. A primary outcome from these benchmarks is the poor performance of current models on out-of-distribution (OOD) tasks. The Benchmarking-VLA-VLM paper notes this as a "Poor OOD cultural ability," and the V-MAGE assessment concludes that current MLLM "ability is not good," highlighting this as a major challenge \cite{Benchmarking-VLA-VLM, v-mage}.

Within these evaluations, researchers rely on a suite of metrics. General metrics such as \texttt{pass@k} are commonly used to measure success over repeated trials. More specific to GUI agents, metrics like \texttt{Invalid\%} track the proportion of invalid actions, a critical diagnostic for models struggling to produce valid outputs \cite{Benchmarking-VLA-VLM}. To properly evaluate agents on long-horizon tasks, benchmarks like ORAK also employ a mix of reward types, including \textbf{Dense Rewards} (for continuous feedback) and \textbf{Auxiliary Rewards} (e.g., for "correct format output") to guide and assess complex behaviors \cite{ORAK}.

Beyond simple success rates, recent work has introduced more sophisticated systems for comparative and granular assessment. To address the difficulty of comparing models across different games, V-Mage introduces a \textbf{Dynamic ELO system} to standardize agent performance \cite{v-mage}. The same framework also moves beyond a single score by using \textbf{"Unit Tests for Core Visual Abilities."} This method provides a granular assessment of MLLM failures, identifying specific failure modes such as errors in \textbf{positioning}, \textbf{direction}, \textbf{identification} (e.g., seeing paths as obstacles), and a "fight" between correct reasoning and incorrect execution \cite{v-mage}.

\subsection{Agentic Modules and Stability in Long-Horizon Tasks}

For an assistant to be a reliable, long-term partner, it must employ a set of \textbf{agentic modules} to manage complex tasks and mitigate error accumulation. While single-step actions are relatively simple, maintaining stability over long-horizon tasks requires a robust architecture. Recent literature has synthesized this into an "agentic chain" of planning, skills, reflection, and memory, which are essential for handling complex, sequential decision-making \cite{llm-ga, gp-agents}.

The most critical of these modules are \textbf{planning and reflection}, which work in a continuous loop. The CRADLE framework provides a concrete example of this chain, with modules for \textbf{"Info Gathering → Reflection → Skill Curation → Action Planning → Task Inference"} \cite{CRADLE}. This aligns with surveys like OS-Agents, which also identify adaptive planning based on \textbf{environmental feedback} as a cornerstone of modern agent design \cite{os-agents}. This feedback loop is heavily reliant on \textbf{memory}. Both short-term and long-term memory are critical for stability, whether implemented as memory scaffolding in lmgame-bench or as a core component of task planning in ORAK \cite{lmgame-bench, ORAK}. In practice, this includes recording actions (e.g., logs, navigation, form data) to provide a persistent context for the agent \cite{os-agents}.

To make long-term planning tractable, agents also rely on a curated set of \textbf{atomic skills}. This concept is widely adopted, with surveys like Agent-AI identifying "Macros (or skills)" as a common feature \cite{agent-ai}. The CRADLE framework implements this directly with its \textbf{"Skill Curation"} module, which dynamically generates and updates re-usable skills to handle complex, long-term tasks \cite{CRADLE}. A key new direction for agents is \textbf{Self-Evolution}, or the ability to automatically assemble new skill-sets for novel tasks \cite{llm-ga}. The UI-Venus report provides a practical implementation of this with a \textbf{"Self-Evolving Trajectory History Alignment"} framework. This method acts as automated reflection, allowing the agent to refine its own reasoning and "thought-action pairs," which leads to "more coherent planning" \cite{ui-venus}.

\subsection{Learning Paradigms for Robust Agent Training}

A stable agent architecture must be supported by a robust training paradigm that bridges the gap between single-step imitation and sustained, interactive assistance. While many foundational models are trained via supervised imitation, this paradigm struggles with the long-horizon, multi-step nature of interactive tasks. Consequently, modern agent systems are increasingly applying \textbf{Reinforcement Fine-Tuning (RFT)} on top of pre-trained models. This approach, as demonstrated in works like UI-Venus and AgentGym-RL, has proven essential for adapting agents to complex, sequential decision-making \cite{ui-venus, agent-gym-rl, rl-fine-tune-driving}. As an alternative, offline learning paradigms like the \textbf{Decision Transformer (DT)} are also explored. Frameworks like R2-Play use this approach, framing the task as sequence modeling rather than reinforcement learning, which can be effective for learning from static datasets of expert behavior \cite{r2-play}.

This shift to more advanced training is enabled by sophisticated reward shaping and iterative refinement. To overcome sparse rewards, systems employ a combination of \textbf{dense rewards} (feedback at each timestep) and \textbf{auxiliary rewards} (feedback for sub-goals or correct formatting). This reward structure is shown to accelerate learning and improve task performance, especially in complex, long-horizon scenarios \cite{agent-gym-rl, rl-fine-tune-driving}. This is then combined with iterative \textbf{feedback loops} that allow the agent to refine its strategy over time, such as the "progressive interaction scaling" proposed by AgentGym-RL or the "self-evolving trajectories" of UI-Venus \cite{agent-gym-rl, ui-venus}.

Finally, a critical component for reliability is the use of \textbf{structured action generation} and \textbf{grounded task execution}. To reduce errors, agents are trained to generate actions in a highly structured form (e.g., JSON) rather than as free-form text \cite{ui-venus}. This is often combined with "Grounded" task execution, which explicitly ties an agent's actions to the UI or game state (e.g., "right-click 'Context'"), a method that helps to reduce ambiguity and prevent the model from hallucinating invalid actions \cite{agent-ai}.

\subsection{Challenges and Future Directions}

Despite the rapid progress in agent capabilities, several persistent challenges must be addressed to develop truly robust, real-time AI companions. The most immediate of these is \textbf{latency}, which is critical for a seamless user experience in interactive entertainment. Research into on-device processing, such as the UI-Venus framework, aims to solve the "unacceptably long inference latency" of large models \cite{ui-venus}. This is a non-trivial requirement; studies in real-time human-AI coordination have empirically demonstrated that latency beyond approximately 100ms is perceived as lag and significantly degrades the sense of fluid cooperation \cite{liu2023llm}.

Beyond performance, significant questions of \textbf{safety and robustness} remain. For an agent to be trusted with any level of control, it must be able to handle dangerous actions and critical errors. The OS-Agents survey identifies "Error Recovery Mechanisms" (such as rollbacks) and "Human-in-the-Loop Control" (such as confirmation mechanisms) as major unsolved challenges for the field \cite{os-agents}. This need for alignment is a key research area, focusing on robustifying agents to ensure safe "transfer" to new situations and prevent unintended consequences \cite{zubia2024robustifying}.

A primary cause of such errors is the agent's difficulty with \textbf{out-of-distribution (OOD) generalization}. Both V-Mage and the Benchmarking-VLA-VLM paper identify OOD generalization as a critical failure point \cite{v-mage, Benchmarking-VLA-VLM}. V-Mage, for example, highlights that vision-only models struggle significantly to adapt to new visual scenarios \cite{v-mage}. This finding is not limited to vision; the Benchmarking-VLA-VLM study found that all evaluated models (VLAs and VLMs) had "significant limitations in zero-shot generalization to OOD tasks," noting that their performance was heavily influenced by factors like action representation and prompt engineering \cite{Benchmarking-VLA-VLM}.

Recent surveys also identify that current agents, while proficient in simple tasks, still struggle with two key failures: long-horizon tasks easily fail as errors accumulate over time, and agents have low fine-grainedness or resolution, making it difficult to perform precise actions \cite{mllm-gui, os-agents}. These challenges collectively define the \textbf{future directions} for the field. As outlined in the CRADLE framework, research must prioritize enhancing an agent's ability to handle complex, multi-step tasks while ensuring real-time reliability. This requires enhancing \textbf{multi-modal capabilities}, improving \textbf{accuracy} in fine-grained control, and solving the "prohibitive inference latency" of current systems, all while ensuring agents remain resilient in unseen environments \cite{CRADLE}.

\subsection{Synthesis and Gaps in the Literature}

This review reveals that while significant progress has been made in functional agent control, a clear and actionable gap exists in the literature regarding unified, companion-style assistants. Current research is largely bifurcated, prioritizing either high-performance task completion or the fundamentals of relational AI, with a significant lack of academic frameworks that effectively merge the two. For instance, while AI systems show growing competence in task execution, recent work such as the \textbf{Emotion AWARE} framework highlights that integrating "multi-granular and explainable" \textbf{emotional intelligence} remains a complex, unsolved challenge—a feature largely absent in current agent architectures \cite{gamage2024emotion}. This gap is evident in state-of-the-art functional systems; frameworks like UI-Venus have made strides in performance via RFT and structured actions, but their architecture is exclusively focused on functional task completion and does not address this relational component \cite{ui-venus}.

This project is therefore positioned to fill this gap by creating an assistant that architecturally blends both companionship and agency. In doing so, it will also address two other persistent challenges in the field: \textbf{real-time adaptability} and \textbf{cross-game transferability}. The former remains a recognized challenge, with benchmarks such as lmgame-bench being specifically designed to test the limitations of current models in spatiotemporal and long-context reasoning \cite{lmgame-bench}. The latter, the difficulty of creating \textbf{game-agnostic agents}, has been pointed out by numerous studies. Both the ORAK benchmark, which highlights the difficulty of evaluating agents across diverse games, and the CRADLE framework, which was built to solve the reliance on proprietary game APIs, confirm that cross-game transferability is a key hurdle for the field \cite{CRADLE, ORAK}.

By focusing on a human-homomorphic interface and feedback-driven learning, this project will directly contribute to these unsolved areas, demonstrating a system capable of the deeper, functional partnership that current literature lacks.